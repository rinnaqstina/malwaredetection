import pickle
from sklearn.feature_extraction.text import TfidfVectorizer

# URL sanitization function
def sanitization(web):
    web = web.lower()
    token = []
    dot_token_slash = []
    raw_slash = str(web).split('/')
    for i in raw_slash:
        raw1 = str(i).split('-')
        slash_token = []
        for j in range(len(raw1)):
            raw2 = str(raw1[j]).split('.')
            slash_token = slash_token + raw2
        dot_token_slash = dot_token_slash + raw1 + slash_token
    token = list(set(dot_token_slash))
    if 'com' in token:
        token.remove('com')
    return token

# Load URL model and vectorizer
with open('models/pickle_model.pkl', 'rb') as f:
    url_model = pickle.load(f)

# Load the vectorizer without unpickling the tokenizer
with open('models/pickle_vector.pkl', 'rb') as f:
    url_vectorizer = pickle.load(f)
url_vectorizer.tokenizer = sanitization

# Test URL scanning
test_url = ["http://example.com/malicious"]
url_vector = url_vectorizer.transform(test_url)
prediction = url_model.predict(url_vector)
print("URL Prediction:", prediction)
